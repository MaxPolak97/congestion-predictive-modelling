{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "import lightgbm as lgb\n",
    "import shap\n",
    "\n",
    "sns.set_style(\n",
    "    style='darkgrid', \n",
    "    rc={'axes.facecolor': '.9', 'grid.color': '.8'}\n",
    ")\n",
    "\n",
    "sns.set_palette(palette='deep')\n",
    "sns_c = sns.color_palette(palette='deep')\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = [15, 5]\n",
    "plt.rcParams[\"figure.dpi\"] = 100\n",
    "plt.rcParams[\"figure.facecolor\"] = \"white\"\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load & Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "data = pd.read_pickle('20231108_Dataset_Processed.pkl')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into train, validation, and test sets\n",
    "train_size = 0.75\n",
    "val_size = 0.15\n",
    "test_size = 0.10\n",
    "\n",
    "# First split to separate out the test set\n",
    "train_val, test = train_test_split(data, test_size=test_size, shuffle=False)\n",
    "\n",
    "# Second split to separate out the validation set\n",
    "train, val = train_test_split(train_val, test_size=test_size/(train_size + val_size), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Plotting the data\n",
    "sns.lineplot(data=train['transformer_load'], ax=ax, label='Training Set', color=sns_c[0])\n",
    "sns.lineplot(data=val['transformer_load'], ax=ax, label='Validation Set', color=sns_c[1])\n",
    "sns.lineplot(data=test['transformer_load'], ax=ax, label='Test Set', color=sns_c[2])\n",
    "\n",
    "# Adding vertical lines for the start of validation and test sets\n",
    "ax.axvline(val.index[0], color='black', ls='--')\n",
    "ax.axvline(test.index[0], color='black', ls='--')\n",
    "\n",
    "# Setting up the legend and titles\n",
    "ax.legend()\n",
    "plt.title('Data Splitting')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Transformer Load')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the data for LightGBM\n",
    "target = 'transformer_load'\n",
    "train_data = lgb.Dataset(train.drop(columns=target), label=train[target])\n",
    "val_data = lgb.Dataset(val.drop(columns=target), label=val[target])\n",
    "test_data = lgb.Dataset(test.drop(columns=target), label=test[target])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial hyperparameters\n",
    "lgb_params = {\n",
    "    'objective': 'quantile',\n",
    "    'metric': 'quantile',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'seed': 42,\n",
    "    'learning_rate': 0.1,\n",
    "    'num_leaves': 16,\n",
    "    'max_depth': -1,\n",
    "    'min_gain_to_split': 0,\n",
    "    'bagging_fraction': 0.9,\n",
    "    'bagging_freq': 1,\n",
    "    'feature_fraction': 0.9,\n",
    "    'lambda_l1': 0.01,\n",
    "    'lambda_l2': 0.01,\n",
    "    'force_col_wise': 'true'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training models for each quantile\n",
    "quantiles = [0.05, 0.5, 0.95]\n",
    "models = {}\n",
    "\n",
    "for quantile in quantiles:\n",
    "    print(f\"Training model for quantile: {quantile}\")\n",
    "    lgb_params['alpha'] = quantile\n",
    "    model = lgb.train(lgb_params,\n",
    "                      train_data, \n",
    "                      num_boost_round=400,\n",
    "                      valid_sets=[train_data, val_data], \n",
    "                      callbacks=[lgb.early_stopping(stopping_rounds=40)]\n",
    "                      )\n",
    "    models[quantile] = model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explainability Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine SHAP values \n",
    "explainer = shap.TreeExplainer(models[0.5])\n",
    "shap_values = explainer.shap_values(train.drop(columns=['transformer_load']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the SHAP summary plot\n",
    "shap.summary_plot(shap_values, train.drop(columns=['transformer_load']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary, wind speed is a key driver of the model's predictions, with features such as 'windspeed_squall', 'windspeed_100m_3h_mean', and 'windspeed_10m' being the most influential. Additionally, radiation stands out as another critical factor. It's evident from the positive SHAP values and high feature values that increased readings of these wind speed and radiation features tend to elevate the predicted outcome. **This indicates that the model associates stronger wind conditions and higher radiation levels with a higher transformer load.** This makes sense, as wind and solar power are both renewable energy sources that supply more power when conditions are favorable (e.g. higher radiation and wind speed)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error Anylysis\n",
    "We can see that from the metric scores and the visualizations, the model generalizes well on the validation data, but the performance drops on the test data. This makes sense as the time series data is highly dynamic and underlying patterns can change quickly.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions on the train, validation, and test sets for upper, median, and lower quantiles\n",
    "train_preds_lower = models[0.05].predict(train.drop(columns=['transformer_load']))\n",
    "train_preds = models[0.5].predict(train.drop(columns=['transformer_load']))\n",
    "train_preds_upper = models[0.95].predict(train.drop(columns=['transformer_load']))\n",
    "\n",
    "val_preds_lower = models[0.05].predict(val.drop(columns=['transformer_load']))\n",
    "val_preds = models[0.5].predict(val.drop(columns=['transformer_load']))\n",
    "val_preds_upper = models[0.95].predict(val.drop(columns=['transformer_load']))\n",
    "\n",
    "test_preds_lower = models[0.05].predict(test.drop(columns=['transformer_load']))\n",
    "test_preds = models[0.5].predict(test.drop(columns=['transformer_load']))\n",
    "test_preds_upper = models[0.95].predict(test.drop(columns=['transformer_load']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating MAE, RMSE, MAPE, MASE, and mdRAE\n",
    "metrics = {}\n",
    "for split, actual, predicted in [('Train', train['transformer_load'], train_preds),\n",
    "                                 ('Validation', val['transformer_load'], val_preds),\n",
    "                                 ('Test', test['transformer_load'], test_preds)]:\n",
    "    \n",
    "    r2 = r2_score(actual, predicted)\n",
    "    mae = mean_absolute_error(actual, predicted)\n",
    "    rmse = np.sqrt(mean_squared_error(actual, predicted))\n",
    "\n",
    "    metrics[split] = [r2, mae, rmse]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the metrics to a DataFrame for better visualization\n",
    "metrics_df = pd.DataFrame(metrics, index=['R2', 'MAE', 'RMSE']).T\n",
    "metrics_df.rename_axis('Split', inplace=True)\n",
    "metrics_df.reset_index(inplace=True)\n",
    "metrics_df['Model'] = 'Baseline'\n",
    "metrics_df.set_index(['Model', 'Split'], inplace=True)\n",
    "\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_actual_vs_predicted(datasets, predictions, titles, x_label='Actual', y_label='Predicted'):\n",
    "    \"\"\"\n",
    "    Plots actual values against predictions with ideal diagonal lines for each dataset.\n",
    "\n",
    "    :param datasets: List of datasets containing the actual values.\n",
    "    :param predictions: List of predictions corresponding to each dataset.\n",
    "    :param titles: List of titles for each subplot.\n",
    "    :param x_label: Label for the x-axis.\n",
    "    :param y_label: Label for the y-axis.\n",
    "    \"\"\"\n",
    "    # Number of datasets\n",
    "    n_datasets = len(datasets)\n",
    "\n",
    "    # Setting up the plot\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=n_datasets, figsize=(5 * n_datasets, 8))\n",
    "\n",
    "    # Looping through datasets and predictions to create each subplot\n",
    "    for i in range(n_datasets):\n",
    "        sns.scatterplot(x=datasets[i]['transformer_load'], y=predictions[i], ax=axes[i], s=10)\n",
    "        \n",
    "        # Adding an ideal (diagonal) line\n",
    "        max_val = max(datasets[i]['transformer_load'].max(), max(predictions[i]))\n",
    "        axes[i].plot([0, max_val], [0, max_val], color='red', lw=1, linestyle='--')\n",
    "\n",
    "        # Adding titles and labels\n",
    "        axes[i].set_title(titles[i])\n",
    "        axes[i].set_xlabel(x_label)\n",
    "        axes[i].set_ylabel(y_label)\n",
    "    \n",
    "    fig.suptitle(\"Actual vs Predicted Transformer Load\", fontsize=16)\n",
    "    plt.tight_layout()\n",
    "\n",
    "# Example usage for train, val, and test sets\n",
    "plot_actual_vs_predicted(\n",
    "    [train, val, test], \n",
    "    [train_preds, val_preds, test_preds], \n",
    "    [\"Train Set\", \"Validation Set\", \"Test Set\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_errors(dataset, predictions):\n",
    "    \"\"\"\n",
    "    Calculates errors between the actual values and predictions.\n",
    "    :param dataset: The dataset containing the actual values.\n",
    "    :param predictions: The predicted values.\n",
    "    :return: A Series containing the errors.\n",
    "    \"\"\"\n",
    "    return predictions - dataset['transformer_load']\n",
    "\n",
    "# Calculating errors for each dataset\n",
    "train_errors = calculate_errors(train, train_preds)\n",
    "val_errors = calculate_errors(val, val_preds)\n",
    "test_errors = calculate_errors(test, test_preds)\n",
    "\n",
    "# Creating a DataFrame for plotting\n",
    "error_data = pd.DataFrame({\n",
    "    'Error': pd.concat([train_errors, val_errors, test_errors]),\n",
    "    'Dataset': ['Train'] * len(train_errors) + ['Validation'] * len(val_errors) + ['Test'] * len(test_errors)\n",
    "})\n",
    "\n",
    "# Plotting errors using seaborn's violinplot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.violinplot(x='Dataset', y='Error', data=error_data, hue='Dataset')\n",
    "plt.title('Error Distribution Across Train, Validation, and Test Sets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions_with_confidence_interval(dataset, preds_lower, preds, preds_upper, title):\n",
    "    \"\"\"\n",
    "    Plots the real transformer_load vs predicted transformer_load with 90% confidence interval.\n",
    "\n",
    "    :param dataset: The dataset containing the real transformer_load.\n",
    "    :param preds_lower: Predictions for the lower bound (5th percentile).\n",
    "    :param preds: Median predictions (50th percentile).\n",
    "    :param preds_upper: Predictions for the upper bound (95th percentile).\n",
    "    :param title: Title of the plot.\n",
    "    \"\"\"\n",
    "    # Creating a Plotly figure\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Adding the real transformer load\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=dataset.index, y=dataset['transformer_load'], \n",
    "        mode='lines', name='Actual', \n",
    "        line=dict(color='blue')\n",
    "    ))\n",
    "\n",
    "    # Adding the predicted transformer load\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=dataset.index, y=preds, \n",
    "        mode='lines', name='Predicted', \n",
    "        line=dict(color='red')\n",
    "    ))\n",
    "\n",
    "    # Adding the lower bound of the confidence interval\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=dataset.index, y=preds_lower, \n",
    "        mode='lines', line=dict(width=0), \n",
    "        showlegend=False\n",
    "    ))\n",
    "\n",
    "    # Adding the upper bound of the confidence interval and filling the area\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=dataset.index, y=preds_upper, \n",
    "        mode='lines', line=dict(width=0), \n",
    "        fill='tonexty', fillcolor='rgba(255, 0, 0, 0.2)', \n",
    "        name='90% confidence interval'\n",
    "    ))\n",
    "\n",
    "    # Adding layout specifications\n",
    "    fig.update_layout(\n",
    "        title={\n",
    "            'text': f'<b>{title}<b>',\n",
    "            'y': 0.9,\n",
    "            'x': 0.5,\n",
    "            'xanchor': 'center',\n",
    "            'yanchor': 'top',\n",
    "            'font': {'size': 20, 'color': 'black', 'family': \"Arial, sans-serif\"}\n",
    "        },\n",
    " \n",
    "        xaxis_title='Date',\n",
    "        yaxis_title='Transformer Load',\n",
    "        hovermode=\"x unified\"\n",
    "    )\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days_15 = 15 * 96\n",
    "plot_predictions_with_confidence_interval(\n",
    "    test[0:days_15], \n",
    "    test_preds_lower[0:days_15],\n",
    "    test_preds[0:days_15], \n",
    "    test_preds_upper[0:days_15], \n",
    "    \"Short-term Forecasting of Transformer Load (1st 15 Days of Test Set)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model performs well on the first part of the test data as the predictions follow the same pattern as the actual values. The confidence interval is generally narrow, which means that the model is confident in its predictions, but also covers whenever the median model is wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions_with_confidence_interval(\n",
    "    test[-days_15:-1], \n",
    "    test_preds_lower[-days_15:-1],\n",
    "    test_preds[-days_15:-1], \n",
    "    test_preds_upper[-days_15:-1], \n",
    "    \"Short-term Forecasting of Transformer Load (Last 15 Days of Test Set)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model performs poorly on the last part of the test data and the model is likely drifting. We can also see that the confidence interval have become wider, which means that the model is not confident in its predictions (as indicated by a lower R2 score as well)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Decision Making to Prevent Congestion\n",
    "We can now aid operators in making informed decisions to manage the grid efficiently and prevent congestion. With the quantile regression model, we can predict the load of a transformer with a certain probability and whenever the prediction exceeds the transformer's capacity of 1, the operator should take action to prevent this from happening. The following actions can be taken:\n",
    "\n",
    "1. The operator can store the energy in batteries if the load is predicted slightly above the capacity.\n",
    "2. Sell the energy to other regions (GOPACS) if the load is predicted to be much higher than the capacity.\n",
    "3. Shutdown the wind turbines and solar panels when we can't sell the energy to other regions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions & Next Steps\n",
    "- **Quantile Regression** is a good approach to model time series data as it can capture the **certainty of the predictions** compared to only single output regressionthat is hard to comprehend. \n",
    "- The **model's behavior is consistent with the domain knowledge** as it uses wind and radiation features to predict the transformer load.\n",
    "- The model didn't overfitted but is likely **susceptible to drift**. It is highly recommended to retrain the model periodically to avoid this.\n",
    "- It might be worthwhile to do some additional **hyperparameter tuning & automatic feature selections** methods to improve the model performance.\n",
    "- At this point we are unable to predict ahead as we use the wind and radiation features that are not available in the future. We could make predictions ahead by using only the **historical futures** (e.g. derived and lagged features) or get the forecasted wind and radiation values from another model. Creating a forecasting model using only the historical futures is shown in the next notebook `03_Forecasting.ipynb`.\n",
    "- It would also be a good idea try other models that can direcly predict future values such as **statistical models** (e.g. ARIMA, SARIMA, etc.) and **neural networks** (e.g. LSTM, Transformers, etc.). An hybrid approach would be most feasible to balance the performance and interpretability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
